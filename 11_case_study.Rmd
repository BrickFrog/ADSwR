# Case Study: Building the *Valuecheck*

My current employer (funda)[https://www.funda.nl/] is the market place for selling and buying homes in the Netherlands.
A long standing company wish was to use the data of recent house sales for predicting the current value of all houses in the Netherlands.
We knew that home owners used the asking prices of neighboring houses published on *funda* to keep track of local market trends.
We wanted to facilitate them to translate the recent sale prices in an official estimate of their specific house.
They no longer had to look at the houses that are offered, out statistical model had already done that.
Moreover, they did not make the translation of offered houses to their own house informally, the model determined which characteristics of a house matters and which not.
A final advantage was that we could use of the selling prices instead of the asking prices, these are not shown on the website.
So the product reflected what their house could be sold for, instead of what the typical asking price of their home was.

To create what eventually would become the *Valuecheck*, a colleague data scientist and me joined an existing Scrum team.
This team comprised of two backend developer, a frontend developer, a UX designer, and a product owner. 

## Trying Scrum

The teams was very experienced with using Scrum and had all the workflows in place, so it made sense to try to fit in our tasks into this framework.
At first this worked out quite well, because the first set of tasks we had to complete were essentially software tasks.
Setting up a server, build a first query so we had a modelling set, splitting into train and test, and doing some data cleaning.
These tasks were scopeable, we could estimate the time we needed to complete them quite accurately and they had a clear definition of done.
Then the model building started and we got more and more trouble fitting the tasks into the tight Scrum methodology.
We could not tell what the model would like in two weeks time, it depended on the relationships we would find in the data.
We certainly could not give estimates what the model quality would be then (measured in a an agreed-upon statistic).

## Moving to Kanban

Having scopeable tasks is essential for building proper Scrum sprints.
As a team you have to commit to what you are going to complete in the upcoming two weeks.
No longer being able to do that, we could not be part of the Scrum rythm anymore.
We found the alternative in moving the data science tasks to a separate Kanban board, stepping out of the Scrum cycles.
The circular nature of data science, as discussed in Chapter 5, does not lend itself well for tight planning. 
We started with a Kanban board with six lanes *to do* - *test hypothesis* - *code review hypothesis* - *update model* - *code review update model* - *done*.
This reflected a full cycle of researching a hypothesis and updating the software. 
It worked quite well, however, a part of the tasks did not reach the finish line, because the hypothesis tested appeared not to improve the model.
This problem does not exist if the research part and the software part are split in separate tasks, the software task is only created if the research indicated the model could be improved. 
We never did this during this project, but this insight improved our workflow in subsequent projects.

## Building an MVM for the MVP

Building a predictive model that is part of a dedicated product is both challenging and rewarding.
Too often data science projects are initiated as a proof of concept, without a clear vision on how to implement if the prediction can be succesfully done. 
Knowing from the start that the model is going to be used, is very motivating.
On the other hand, this means that you need constant allignment with the team that develops the product around the predictions.
The houses offered for sale on *funda* have many charecteristics filed, giving us a rich feature set to work with.
However, as an MVP we wanted to present the users with an estimation, without them having to fill in all kinds of characteristics of theirs house.
Developing a product from static house predictions is far less complex and time consuming than from a dynamic model with adjustable inputs.
Both from modelling and a software perspective.
This implied that we could only use features that are freely available for every house in the Netherlands.
Luckily, this was true for the two most important features, location and time.
Also the surface area of the houses was available in a public database.
From this we started to build our initial prediction models.
First using simple regression model to create a baseline.
We have a strong preference for statistical models over machine learning algorithms, because they not only give us predictions, but also insight.
However, we needed some decent predictions fast, and it was clear we needed to exploit some nonlinear relationships.
We therefore used some ensemble models that gave far superior predictions to regular regression.

Already it was decided, we would only release the MVP in geographical areas in which the MVM performed well enough.
This is called a *soft launch*, release the product without giving it too much noise for a selected group.
Even then we not quite made the minimal performance goals we set ourselves. 
However, we could include a categorical feature and simply export the predictions for every level of the feature for every house, as long as there were not too many levels.
The type of the house (appartment, on of several Duthch types of houses) appeared another crucial predictor.
Finally, we wanted to show lower and upper bounds to a prediction, not only giving a point estimate.
After some research we were able to this with random forests, that were trained on the desired quantiles.
Predictions were exported in csv files, a frontend and backend were built around these.
Doing a prediction on the website was just a simple lookup.

## Improving the Product

From the start users could provide us with feedback, using a simple *thumbs up, thumbs down* and if they wished subsequent comments.
Of course you want your work to be liked, but as I quickly learned, in this stage the best feedback is negative feedback.
You know the product is barely good enough at this point in time, both from a software and a data science perspective.
Negative feedback indicates people care about the product and it bothers them it does not fully meet their expectations.
Moreover the feedback can point to directions that gives the biggest improvements in user satisfaction when improved.
If it appeared that the users did not care about the product in the first place, the project could be killed and little resources were wasted on it.
Fortunately, users did care, so we went ahead and started improving.

###  Interactivety

Using an interactive model instead of the static MVM would improve the product in several ways.
It enabled us to use features in the model that were not publicly avaible, the user could enter them.
Also, we knew that the data on house surface area was not of consistent quality.
Comparing them to the "real" data in our data bases for houses that were placed on our website indicated that they could be off in both directions.
In fact, for the MVM we used a model to predict the "real" surface area based on the public data.
When the product changed to interactive, the user could correct the prefilled information if necessarry. 
Finally, it improved transparancy and user experience. 
Having an interactive product meant we had to bring the model to the product, not just the predictions.

### Changing the Model

From the start we saw the problem we were trying to solve would have a natural fit with Bayesian regression models.
Price trends are hugely important in the housing market.
With the *prior - posterior* structure of the Bayesian framework, we could easily update each month.
Making time an implicit variable, rather than a feature we use for modelling.
Secondly, we always wanted to give a lower and upper bound to the predictions, theser are part of the posterior of Bayesian predictions.
The only problem was, we did not have hands-on experience with Bayesian modelling, we just knew the theory behind it.
After extensive discussion if we did not see another way of significantly improving the model, we decided to go for it.
This was a gamble, because we needed a few weeks to refresh our knowledgen and to get our feet wet with *Stan*.
The modelling was basically placed on hold then.
Fortunately, it paid off.
After some experimenting we found a good model structure for our problem, improving the model especially at locations we did not so well before.
Moreover, we could now start to add more features to the model.
This improved both the accuracy of the predictions, the confidence bounds around our estimates shrunk, and improved user experience significantly.

## Productionising the Interactive Model




