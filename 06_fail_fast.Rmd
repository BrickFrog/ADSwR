# Code that Fails

Your code will fail. Not once, not twice, but hundreds, thousands of times in the course of a large data science project. Everybody's code fails, many times. Your code, my code, Hadley Wickham's code. Not because we are bad at what we do, but because it is not possible to foresee all the possible ways the code will be used or all the possible data inputs that the code will run against at the moment we write it. Getting better at programming does not mean you will write fault-free code (although you will make fewer errors as you grow more experienced), but writing code that fails fast and informatively. 
Writing such code means that you will spent more time developing than with the *random walk* method described in the previous chapter. This is an investment that will pay off big time as your project grows in complexity. 

But before we look into how to write code that fails fast and informative, why is this important for Agile Data Science? High quality code is part of the twelve principles, but this is a means to an end. The essence of Agile is continuous delivery. Updating and shipping your product everytime you made progress can only be done when your code is adjustable. As the project advances, the end-to-end product will grow in complexity. Sooner rather than later it will reach a point in which you cannot have a full overview of all the aspects of the product anymore. When introducing a new feature you want to make sure all the other elements in the code base keep doing what they were designed for. If not, you want to be instructed clearly what is going wrong so you can either adjust the newly introduced feature or modify the existing elements so it works with the new feature. If everytime you want to make a change the whole thing comes tumbling done without informing you what goes wrong, you cannot achieve the objective of fast, continuous delivery.

## Assumptions to your Data

Automation means not calling the functions yourself anymore. You just flip the switch and the whole thing is set in motion. This means that you are no longer get the inmediate feedback you are used to when calling the functions yourself. When something goes wrong the whole thing just breaks with an error message. When you are lucky the error message is informative and you are quick to find the bug. When you are not, you may be confronted with `object of type 'closure' is not subsettable`, you may have grumpy afternoon ahead. Best to not depend on luck, but make sure you will always get a proper indication what went wrong. 

### Type Checking

Most languages using functional programming are strongly typed. This means that for every argument that a function takes, its datatype is specified at function creation, as well as the datatype that the output takes. When the function is than called it is first checked if the data on which it is called is of the correct type, or at least can be forced to the specified type. If not, it will break inmediately and will tell the user why. R is weakly typed, the data type of the function arguments are not specified. When a function is called it just has a go on the objects that are fed to it. The function only breaks when somewhere in the body another function gets called with an invalid data type. Take the following for instance:

```{r}
add_two_numbers <- function(x, y) {
  print("Reaching this")
  print("and this")
  print("Doing some random other stuff")
  x + y
}
```

Now, what will happen if we accidently call it on a string? It will only break when we hit the `+` operator, all the code before it runs. The error message it gives us, is not so informative that we inmediately figure out what goes wrong.

```{r, error = TRUE}
add_two_numbers(42, "MacGyver")
```

Pfff, what is a binary operator again? When this functions is part of a framework in which higher order functions call lower order functions you may be up for an hour or two of sifting through your code to locate the bug. Fortunately type checking is very easily add to a function by asserting all argument data types in `stopifnot`. 

```{r, error = TRUE}
add_two_numbers <- function(x, y) {
  stopifnot(is.numeric(x), is.numeric(y))
  print("Reaching this")
  print("and this")
  print("Doing some random other stuff")
  x + y
}

add_two_numbers(42, "MacGyver")
```
We are now specifically informed, which argument does not meet the assumptions and in what the name of the subfunction is. 

### Column Validation

Central to most data products will be the modification of data frames. Probably most functions you'll create are very specific to your project and will be called only once. For these cases you want to save yourself the overhead of making each function completely generic by parametrising the column names that are used. Say you want to add the log of the target to the data frame and you create the following function.
```{r}
add_log_target <- function(x) {
  stopifnot(is.data.frame(x))
  x$target_log <- log(x$target)
  x
}
```
Now what if `target` was mistakingly removed from `x` in the step preceding this one in the product. Will it throw an informativer error?

```{r, error = TRUE}
add_log_target(mtcars)
```
Uhh, no not exactly. Another of those R error beauties that will leave you clueless for a time. It is a good idea to check if all the columns that are required for the operations in the function are present before starting them. I use this little function,

```{r, error = TRUE}
df_has_cols <- function(x, cols, func) { 
  stopifnot(is.data.frame(x))
  if(!all(cols %in% colnames(x))) {
    not_present <- setdiff(cols, colnames(x))
    stop(paste(not_present, collapse = ", "), " missing from the data frame in function ", func[[1]])
 }
}
```
which is added to each function that uses a data frame

```{r, error = TRUE}
add_log_target <- function(x) {
  df_has_cols(x, "target", match.call())
  x$target_log <- log(x$target)
  x
}
add_log_target(mtcars)
```
(`match.call()` has to be added so it will return the name of the function that breaks for easy debugging).

### Data Validation

Where the first two checks are about the structure of the inputs, data validation tests the assumptions to the data itself. Variables that cannot contain missing values, variables that must be strictly positive, or characters that can only take a limited number of values. If you have any such assumptions to your data, you do best to check them before you unleash your functions on it. You could use external packages such as `validate` or `recipes`, or you can write them yourself. The `stopifnot` function can be used to check any assumption you have, as long as you can create a single logical for it. In the above example, since we are taking the log of `target` and it is the target variable it must be strictly positive and nonmissing.

```{r}
add_log_target <- function(x) {
  df_has_cols(x, "target", match.call())
  stopifnot(all(!is.na(x$target)), all(x$target > 0))
  x$target_log <- log(x$target)
  x
}
```

## Unit Testing

The above, type checking, column validation, and data validation, test assumptions to the data going into the functions. To data scientists learning about these usually makes a lot of sense, because most who worked on a larger product without these has been bitten quite a bit. Moreover, these measures are quick to implement by just inserting one or two lines of code at the beginning of a function. Unit testing your code does not come so natural to data scientists, unless they have a background in software development. Writing tests might seem as a pointless, time consuming exercise at first sight. This is because its benefits are not inmediately obvious to someone who never applied used them. Still, you should make it part of your software development routine, even when you are the only one using the software you write.

If you are unsure what unit tests are or how to do them in R, please read [the chapter in *R packages*](http://r-pkgs.had.co.nz/tests.html) carefully. Hadley Wickham 

### Externalising Assumptions

Reading someone else's code is demanding, even when written by an excellent programmer. At the moment of writing the programmer is fully submerged in the problem, accommodating for all the inputs she foresees the code can be confronted with. Reading her code is tough because you have to recover from it what the problems are it tried to solve. After you wrap up a part of the project and move on to work on something else, the wrapped up code quickly becomes as if written by someone else. Unit tests capture all the cases the programmer envisioned the unit of code should handle. It is a service to the future collaborator, who very well could be the same person, so she doesn't have to mentally fight her way into the code. As soon something is changed to the function such that it no longer does everything it was designed to do, the unit test informs us. When there are no

* slow down - think through scenarios

