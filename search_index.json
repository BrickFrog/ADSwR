[
["index.html", "Agile Data Science with R A workflow 1 Working without a Workflow 1.1 What this Text is About 1.2 Writing out Loud 1.3 Intended Audience", " Agile Data Science with R A workflow Edwin Thoen 1 Working without a Workflow Not even too long ago, when I was starting my career as a data scientist, I did not really have a workflow. Freshly graduated from an applied statistics master I entered the arena of Dutch business, employed by a small consulting firm. Neither the company I was with, nor the clients I was working for, nor myself had an understanding of what it meant to implement a statistical model or a machine learning method in the real world. Everybody was of course interested in this “Big Data” thing, so it did not take long before we I could start at clients, often without a clear idea what I was going to do. When we finally came to something that looked like a project, I plunged into it. Eager to deliver results quickly I loaded data extracts into R and started to apply all kinds of different models and algorithms on it. Findings ended up in the code comments of the R scripts, scripts that often grew to hundreds or even thousands of lines. To still have somekind of an overview I numbered the scripts serially, but this was about all the system I had. Soon I found myself amidst dozens of scripts and data exports of intermediate results that were no longer reproducible. The R session I was running ad infinitum was sometimes mistakenly closed, or it crashed (which was bound to happen as the memory used grew). I spent hours or even days to recreate the results when this happened. Deadlines were a nightmare, everything I had done so far had to be loaded, joined and compared at the last moment. More often than not, the model results appeared to be different from the indications in the notes I took earlier, with no idea if I was mistaken earlier, I was using the wrong data now, or some other source of error I was not aware of was introduced. Looking back at these times, I had no clue about the importance of a good workflow for doing larger data science projects. I was saved several times when plug was pulled from the project, due to other reasons. If I was expected to bring the results to production then, it would certainly been a painful demasqué. I learned a great deal since these day, both from other people’s insights and from my own experiences. Writing an R package that was shipped to CRAN enforced me to understand the basics of software engineering. Not being able to reproduce crucial results forced me to start thinking about end-to-end research and model building, controlling all the steps along the way. Last year, for the first time, I joined a Scrum team (frontend, backend, ux designer, product owner, second data scientis) to create a machine learning model that we brought to production using the Agile principles. It was an inspiring experience from which I learned a great deal. My colleagues patiently explained the principles of Agile software development and together we discovered what did and did not work for data science. 1.1 What this Text is About All these experiences culminated in the workflow that we are adhering to at work now and that I think is worthwhile sharing. It is heavily based on the principles of Agile software production, hence the title. We have explored which of the concepts from Agile did and did not work for data science and we got hands-on experience in working from these principles in an R project that actually got to production. This text is split into two parts. In the first we will look into the Agile philosophy and some of the methodologies that are closely related to it (chapters 2 and 3). Both will be related to the machine learning context, seeing what we can get from the philosophy (chapter 4) and what an Agile machine learning workflow might look like (chapter 5). The second part is hands on. We will explore how we can leverage the possibilities in the R software system to implement Agile data science. 1.2 Writing out Loud Data science projects can greatly differ from each other. There are so many variables that make each project unique, there are many situations I have not experienced and there are necessarily many possible aspects of data science projects I am not aware of. In this writing I am relating my own experiences to the theory and best practises of Agile software development, to come up with a general workflow. This means that if I were to write this text in isolation I would be “overfitting” the workflow on about the dozen large data science projects I have done. That is why I need your help. I hope many of you will read the first drafts of this book very critically and relate the content to their own experiences. If you find that parts are incomplete or plainly incorrect, please file an issue. Also, anyone who succesfully completes data science projects must have developed an effective workflow for themselves, even when it is not grounded in a widespread theory such as Agile. I am very interested in the best practises you have developed, even when they don’t fit directly in the framework. File an issue with what you would like to add, if we can’t fit it in the text we can always add it as an appendix or a discussion. This text is meant to be a living thing with the objective of documenting a workflow that yields optimal reproducibility, quick shipping of results and high quality code. The more people share their best practises, the closer we get to this objective. Please follow along on this journey and get involved! Finally, I am not a native of English so fixed typos and style improvements are greatly appreciated. 1.3 Intended Audience The title of this text has four components: Agile, data science, R, and workflow. When you are interested in all four, you are obviously at the right place. This text is not for you if you hope to learn about different algorithms and statistical techniques to do machine learning, more knowledgeable people have written many books and articles on those topics. Also you will not learn anything about R programming. The workflow I present is completely separate from the algorithms you choose and the data preparation tools of your preference, as it focuses on code organisation and delivery. When you use python rather than R, you will still find this text valuable. The first part especially, which focuses on workflow only and is tool agnostic. The larger data science projects I was involved with all had the objective of delivering predictions in some way, thereby you can file them under machine learning. I intend to present a generic workflow, that is also applicable to data science projects that have a different type of delivery, such as automated reports and Shiny applications. You might find machine learning a bit overrepresented in the examples and applications. If you think there is still a misfit between your daily data science practice, please let me know. "],
["agile-in-a-nutshell.html", "2 Agile in a Nutshell 2.1 The Origins of Agile 2.2 The Manifesto 2.3 The Twelve Principles", " 2 Agile in a Nutshell 2.1 The Origins of Agile Agile software development is not a specific methodogy, a process, or a prescription how to do your job. Rather it is a set of beliefs, a philosophy, that should guideline a team developing software in making the best possible decisions. Agile was not created out of thin air, of course, it was very much a reaction of the then ubiquitous approach called Waterfall. In Waterfall large software projects are divided into specific stages, each stage should be completed before the next stage can start. Subsequentially these stages are problem analysis, designing the project, writing the software, testing it and finally implementation and maintenance. Here you can find a clear and neutral introducion into the Waterfall approach. What stands out at Waterfall is delivering complete and faultless software. In order to do so, the centre of gravity of a Waterfall project is the documentation, in which every requirement and aspect of the software is written down meticulously. The underlying conviction is that the software is written faster and is of higher quality when all aspects of it is decided upon upfront. Projects done with the Waterfall method have a major pitfall, however. They can take very long to be completed. The combination of sequentiality and completeness might cause projects to last many years before they get delivered. Each time an error or incompleteness is found in one of the lower stages, the project moves back to the previous stage to fix it. Because of the long duration of the entire process of Waterfall it occured often that the end result was no longer a good fit for the ever changiin market. This that once the product is finally completed, it is no longer a good fit for the market. The problem analysis might be done years ago and the problem has changed in the meantime. Agile has a radical different appoach to software design, by recognising a number of flaws in Waterfall thinking. First, it is impossible to think through all the aspects of a complex design and architecture before writing a line of code. Software has to grow organically instead of being completely designed upfront. Secondly, customers typically don’t really know what they want from the product, until they start interacting with it. Legally it might be a good idea to have all the aspects checked off before getting to work, but it will not keep the customer satisfied. Finally, customers and stakeholders will loose interest and faith if it takes a long time before the works results any tangible results. 2.2 The Manifesto During the nineties several reactions to the cumbersome Waterfall came to being. A number of influential thinkers in the world of software development started thinking what an alternative to the malfunctioning practises could be. Alternative processes were suggested, such as scrum and Xtreme programming. Eventually, in 2001 a group of seventeen came together in Utah and drew up the Manifesto for Agile Software Development. Their just 68-word-long statement is: We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan That is, while there is value in the items on the right, we value the items on the left more. If you think “well, that is rather vague”, it is on purpose. It is not a process you should follow or a methodology that prescribes how you should approach software development. Rather, they are core values that guide the development team with the many choices it makes along the way. At every crossroads the option that is most in line with these values should be selected. 2.3 The Twelve Principles The Manifesto was accompanied by a set of twelve principles that flow from the values. They are more applicable than the four values and are thereby the principle guidelines when making choices. They are (numbering added by me): Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale. Business people and developers must work together daily throughout the project. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity–the art of maximizing the amount of work not done–is essential. The best architectures, requirements, and designs emerge from self-organizing teams. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. "],
["agile-methods.html", "3 Agile Methods 3.1 Scrum 3.2 Kanban 3.3 Scrum vs Kanban", " 3 Agile Methods We learned that Agile is not so much a workflow or a method, but rather a philosophy that should guide us whenever a choice has to be made during the completion of the project. In fact, Agile promotes being critical to strictly following a workflow, we should continously questioning if the way we work makes us optimally follow the Agile values and principles. If this is not the case we should adjust our workflow. This does not mean there are no workflows related to Agile. In fact, a number of workflows are developed by the men (yes, no women were there) who drew up the Agile Manifesto. Following these workflows should make it more likely the team follows the Agile values and principles. Here we look into the two best known; Scrum and Kanban. 3.1 Scrum The most well-known and most-applied workflow is Scrum. Developed in the late eighties and early nineties it is tried and tested methodology. It is no longer just applied in software development, but in the development of many other products as well. Scrum works with sprints, set time units after which a shipeable product should be ready. Most teams use two-week sprints, but it can also be shorter or longer. Teams are completely self-organising, they decide what it will do the next sprint and how they will do it. Tasks to do are gathered on the productlog, they are all formulated such that it is clear how they will add value to the product. These user stories take the form “As I would like to such that I ”. Say that you run a website that sends out newsletter to all its customer, but there is no option for opting out yet. The user story for creating such a functionality could then be “As a subscribed user I would like to be able to opt out for the newsletter such that I only receive information when I want to”. Scrum comes with a set of roles and ceremonies that I will describe below. 3.1.1 Roles A Scrum team has three different roles. The scrum master, its core responsibility is making sure the team will make the sprint goals. To do so, she must have a watchful eye. If the sales manager wants something done and tries to persuade one of the devs at the coffee machine, he is kindly redirected to the product owner to get it on the back log. If some of the team members lack some necessary skills, she will make a plan with them how to acquire them. Also, she will be the organiser and facilitater of the different Scrum meetings. The product owner is responsible for what the product should be. He monitors the needs and desires of the customer, so one of its key responsibilities is stakeholder management. He should know the market and notice when it changes. He decides what the product should look like, by translating feature changes in user stories. Thereby he maintains the product log, ordering the stories in how important they are. Where the product owner decides what should be doen, the team of developers decides how to do it. It is completely self-organising. At the beginning of each sprint it makes an assessment how which stories can be done in the upcoming sprint. Team members usually have different expertises, but the completion of the stories is the responsibility of the entire team. 3.1.2 Ceremonies Four ceremonies (meetings) are part of Scrum cycle. At the beginning of the sprint there is the sprint planning, in which the stories are scoped and the definition of done is determined. During the sprint there is at least one stand-up per day, in which team members quickly share what they are working on and what help they need from each other. When the sprint is done the team organises a sprint review in which it presents what work was completed. Finally, there is the retrospective, in which the team discusses what went well during the last sprint and what can be improved. 3.2 Kanban Kanban is much lighter and less process heavy than Scrum. It does not work with fixed time units, such as the Scrum sprints, but it aims achievin a continuous flow. Just as with Scrum the tasks to do are formulated in user stories, but the commitment is just to one story at a time. Scrum has a rigid commitment to the stories in the sprint, only under very special cirumstances does a team deviate from the sprint. In Kanban stories are gathered on a back log and are continuously ordered in importance. Each time a story is completed the most relevant new story is started. Central is the Kanban board, which can be physical or virtual, that has at least the columns to do, doing, and done, but can be tailored to the wishes of the team. Unlike Scrum there is not official role of Product Owner, still it can be useful to have someone handling the incoming requests. This can be a designated person or a team member who is also doing developing work. The team should not focus on too many tasks at once, everything that is pulled from to do should be worked on and only new stories should be pulled as soon as others have reached done. This assures that the focus is always at the most important task ahead and there is not too much multitasking. A team can even choose to set a cap on the number of stories that can be in each column. The central metric is the amount of time it typically takes to complete a task, the cycle time. Effective Kanban teams have short cycle times, they are able to complete the tasks quickly. They can give estimates when work is done with confidence. Just as with Scrum the entire team is responsible for completing a story, not just the “designated” person for the job. In order to have the tasks completed as quickly as possible team members might fulfill tasks now and then that are a little bit out of their comfort zone. 3.3 Scrum vs Kanban The higlhly structured Scrum and the lightweight Kanban are two workflows that could make a team work more according to the Agile principles. They both aim for continuous shipping of working software instead of working towards one major release. They also both give focus on the part you are working on right now, Scrum by fixing the stories that are done in the sprint, while Kanban limits the number of stories that the team is working on. But there are also some remarkable differences. Scrum the team commits to the stories it selected to do this sprint and the building have to be on fire before it will take on work that is not in the sprint. Kanban only prescribes to limit the number of stories that are in work in progress, finish what you start first than start something new. However, for the story to do next everything can change at any moment. Scrum is quite heavy on the ceremonies, shile Kanban does not prescribe recurrent meetings. Both methodologies are applied with great success and its important to keep in mind that they are means to an end, no religions. The Agile values and principles should be the primary guideline and when selecting one of the workflows you do so because it is the best way to work in a Agile way in the team’s situation. The team should decide for itself what is the best way of working and should be monitor if the choice is still the best as the situation changes. In general, however, it makes more sense to use Scrum when a team is working on the completion of a project, whereas the flexibility of Kanban is best suited for a service team that is dealing with a lot of incoming requests. Sources: https://www.youtube.com/watch?v=2Vt7Ik8Ublw https://agilescrumgroup.nl/product-owner-rol-taken/ https://www.atlassian.com/agile/kanban/ https://agileknowhow.com/2018/03/01/do-we-need-a-product-owner-with-kanban/ "],
["agile-data-science.html", "4 Agile Data Science 4.1 Data Science Waterfall 4.2 The Twelve Principle in the Data Science Context", " 4 Agile Data Science 4.1 Data Science Waterfall As discussed in the previous chapter, Agile is a response to Waterfall methodology that was widely adopted in the eighties and nineties. Many projects following this methodology failed because the long duration of these projects. The world had moved on while the process-heavy steps were completed and either the plug was pulled before the product was finished or the finished product had limited value because it was a misfit to the changed world. In data science, as far as I am aware of, there are no such formal methodologies that are followed by many practioners. However, there are ample testimonies of projects that never reached production and I think this is partially due to suboptimal workflow. Just like Waterfall, data science projects can take many months or even years before the results are productionised or the plug is pulled. The data scientist might want to optimise many asprects of the project to give the best predictions possible or have all the envisioned dashboard functionalities in place, before sharing the results with stakeholders. The code might be poorly organised, leading to a lot of time lost merging differenct scripts. Or there might be unclarities on what to predict or which data sources are in scope, due to lack of communication between stakeholders, business people and data scientists. Whatever the reason, adhering to the principles of Agile can get you more productive and efficient. Here we take the time to interpret the twelve principles in the data science context. 4.2 The Twelve Principle in the Data Science Context Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Just as Waterfall prescribes a complete and fault-free product delivered at once, data scientists might be inclined to only release a machine learning model to production once they are confident its predictions are spot on. This principle is a revolutionary break from Waterfall, you should not wait with releasing software until its perfect, instead get it out in the open when it is just good enough. A common term used for this is the MVP (Minimal Viable Product). After the MVP is released it is closely monitored how users are interacting with it and where the biggest room for improvement is. The biggest possible improvement is then tackled first and a new version is released. This cycle of release, monitor, improve, release is repeated many times, such that the product gets better and better. There is no clear definition of done, instead there is debate if the software can be further improved and if the required investments are worth the effort. The machine learning equivalent to this would be a Minimal Viable Model, a model that is just good enough to put into action. Other data science projects, such as reporting, ETL and apps, might have more software elements in them and less data uncertainty. For these an MVP can be defined. Releasing something that is barely good enough might be scary and counterintuitive to the high standards you have for yourself, but it is preferable over long optimisation before releasing for at least the following reasons: It will keep stakeholders excited. Managers and users of the model who commisioned the data science project are impatient to see results. As the projects drags on without any output they are likely to loose interest and confidence the project will end well. Eventually they might pull the plug or put it on hold before anything reached production. If they can interact with the results soon, even if it is imperfect, will give a totally different dynamic. You will fail fast. There is a wide array of reasons a data science project might fail, such as; the problem appears not be translateable into a model in the first place, the data is not of the quality needed, there appears not be enough historical data, or the necassary relationships simply don’t exist. The sooner you implement the product the sooner lurking problems suface. You will get feedback sooner. This is the main reason Agile wants to implement quickly and then iterate. Lets say you build a churn model which the sales department uses for customer retention. As soon as they start acting on your MVM they find out that the interval in which you predict is too short, many customers already have a subscription with a competing party. Instead of further optimising this model, you focus on predicting a longer time ahead. Or the user of Shiny app realises that the ratio you agreed on is not what she wanted as soon as se she sees the app with the first variable implemented. Before including the twelve other variables you first fix the ratio such it is as she wants it. What an MVM looks like is project-dependent of course, but in many cases it would probably make sense to define it a regular statistical measure. A machine learning model might be replacing a business rule that has been in place for many years, the MVM is then ready as soon as the model outperforms the business rule. Data science products that are not model based might be captured with an MVP, instead of an MVM. For both an MVM and an MVP a possibility is only releasing the product for a subset of your target audience, such as a geographical area or users of a certain age. Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage. This principle comes natural to data scienc, since the outcome of a project typically partially dependent on the relationships discovered in data. The Waterfall approach in which every step of the project is planned would be appear hideous to even the biggest lover of process in the data science context. Keep in mind that flexibility should not only be exercised towards assumptions of your data or the models and algorithms you use. Requirements can also be in the framing of the business problem or the way the results are exposed. The very reason we are releasing early is that customesr can put it to the test and through interaction they discover what they really want. Whatever it is, don’t be lazy or rigid and be prepared to steer in a different direction as soon as the situation requires. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale. Whereas the first principle is about the philosophy of early deployment and iteration, this one is about the frequency of deploying updates. The Scrum framework is really strict in the amount of time that can be spent until the next release. The team commits itself to making certain changes to the product in typically a two-week period. At the end of this period the improvements, small as they might be, are deployed. The Scrum mindset is not totally applicable for data science, especially when there is a lot of data uncertainty, as we will explore in the next chapter. It is for instance not feasible to commit to a time interval for model improvent because it is dependent on yet-to-be discovered relationships in the data. However, it is good to keep in mind that every improvement to the model should be deployed as soon as its ready. This creates momentum and excitement by customers, stakeholders, your teammates and yourself. Business people and developers must work together daily throughout the project. Too often data scientists are operating in isolation. Certainly when projects are of a more explorative nature. Having no connection to the business makes it unlikely your work will ever affect anything outside your computer. If you want your work to mean something it should be alligned with the customers desires. Stakeholder management can be done by a representative of the technical team, such as a product owner. If there is no such role on the team, the stakeholder management is the rsponsibility of the data scientist. A second way the business should be involved is for getting information about underlying processes. Unless the data scientist has extensive knowledge of the business, she needs somebody to validate the found results and answer questions about strange finds. Finally, you might need involve a person with technical knowledge of the data, typically a DBA, who can explain the structure and peculiarities of the data you work with. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. This principle is the antithesis of Waterfall in optima forma. Instead of meticuously describing how the job should be done, just set the goals of the projects and leave it up to the team how these goals should be attainted. Data scientis typically already enjoy this type of freedom for the sheer reason that stakeholders often don’t really understand how the products are built. It can happen that business people get overly involved in the process, they can have a strong opinion on which predictors should be used or how the target should be defined or what the best way is to visualise something in a dashboard. Take their advice at heart but also trust your instincts. If you feel a different approach will yield better results than rely on your expertise. You know about overfitting, multicollinearity, non-converging algorithms, theory behind data visualisation and many other topics the business cannot grasp. If you think you approach is better, take the time to explain why you think a different approach is better (in lay men terms of course). The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. A data science project is rarely done end-to-end by one single person. Data might be made available by a DBA, a backender might expose the product at the website, the frontender builds the interface for interacting with predictions, etc. If possible working with these people directly will speed up decision making and improve allignment. Communication by email or chat programs are often slow. Make an effort to be in the same room with your direct colleagues, for at least a part of the project time. Working software is the primary measure of progress. As long as it is not part of the modelling pipeline you have not attained any results yet. As long as your Shiny app only runs on your laptop, it means nothin. You have not added any value as long as your product is not “out there”. Only when the update to the predictions is fully implemented and the predictions are ready to be consumed by the business, there has been true improvement. And only when you have shipped your MVM app to the company server and Sales is basing decision on it, you have delivered something. All too often the reported improvement in accuracy in research scripts does not hold when it is implemented in the full model pipeline. Sometimes it has been done on just a subset of the data that was conveniently available. Or the new feature was tested in isolation and there is not yet a sense of multicollinearity. There is only one true measure of how well we are currently doing, and it is what we currently have exposed. This implies that as long as there is not end-to-end pipeline in place, we cannot tell how well the model is doing. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. The deadline way of doing data science; the stakeholders and the developers meet, they agree to have a first version of the project ready at a set moment in the future. The sponsors forget about the project until right before the deadline, busy with attending meetings and writing memos. The devdeloper goes to work, having ample time before the deadline there are many things that can be explored. The result is an array of research scripts and intermediate results. Suddenly, as the deadline comes near, all this separate research has to come together. Pulling an all nigther he is able to deliver a result, which is presented to the sponsors. The project is then continued, a new deadline is set, and the cycle starts over. Don’t - do - deadlines. They are a recipe for hastily created, nonreproducible results. They promote a workflow of taking it easy at first, stressing out when the dealine comes near and exhaustion after it. Instead set small goals that are attainable in a short timespan, update project if the results are favorable and set a new small goal. This will result in better quality code, a better grip on the model results and a happier team. Moreover, it will result in a product that is constantly improved, which excites sponsors and users. Continuous attention to technical excellence and good design enhances agility. Most data scientists have much to learn from software engineers as it comes to standards and rigor. What makes data science unique is that a part of the code is written for personal use. It is not meant to ship in the final product, it will never leave the personal computer. Still it is very much of the project. Cleaning of the train data, splitting in train and validation sets, running algorithms that produce the models, doing research on relationships in the data and many more steps are for the data scientist’s eyes only. It is tempting to cut corners when you are the sole user of your own code. Why go to the trouble of writing unit tests and documentation for your functions, as soon it does not do what it is supposed to do you are right there to fix it. At the moment of writing it is obvious what your code is supposed to do and as you run the code against the data you are then working with it is straightforward to see if the program indeed does what it supposed to do. However, three months later you completely forgot the reason you wrote that part and you have no clue why it failed against the refreshed data. You never work alone on a project, even if you are the only person working on it. Always consider future you as a separate person who you respect very much and you want to help to do its job as good as possible. The result of poor qualitiy code is often that separately written parts don’t click easily to create a bigger, more complex system. Cutting corners when solving a problem quickly now will backfire later, with code that is unreliable and that fails without having a sense why it fails. Simplicity–the art of maximizing the amount of work not done–is essential. A machine learning project’s goal is often straightforward, predict y as best you can such that some business goals can be achieved. Other than software development there is not much to choose in which features should and should not be included in the final product (features as in characteristics, not as in predictors). The options how to arrive at predicting y, however, are abundant. The biggest challenge is often “what should I explore next?”. Should we explore another database in which we might find new predictors or should we try a different model on the current predictors which involves some additional preprocessing of the data? We can roughly estimate what the amount of work would be to explore both options, it is, however, very hard to predict what the amount of value is the new part will add. A good rule of thumb is that when in doubt choose the option with the least unknown components. Choose an algorithm you know well over one you have never used in practise. Only tap into a new data source if you are convinced that the options on the current data base are exhausted. Machine learning is a field with rapid developments, it is often tempting to seize the opportunity to dive into a new technique or algorithm. Be critical before doing so, is there really no way to obtain similar results with something already familiar to you? The best architectures, requirements, and designs emerge from self-organizing teams. This is another principle that is a clear antidote to Waterfall. Instead meticulously plan every aspect of the project upfront, let the developers come up with the most important project designs as they go. It is impossible to foresee all the aspects of the software project before implementing it, so trying to come up with before writing code is a guarantee for going back and forth between the planning and implementation stages. Due to the iterative nature of building data science products and the insecurity we have on the relationships in the data, this principle seems quite natural in our context. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. Every development process has its inefficiencies, wether they are unclear goals, poor communication or not having the right priorities. Having to reflect on the process forces you to look critically at all aspects of the project. Inefficiencies can quickly become project features when they exist for a while, the sooner they are tackled the better. Even when you are not in a team following an official methodology such as Scrum or Kanban, you do best in planning regular reflection meetings. Even when you are the only data scientist or even the only development of the team, you should also address your technical issues here. Maybe you are wanting to refactor a certain part of the project for a while but are unsure if it is worth the time. Even though your business colleagues don’t understand the technical aspect of the problem, they can still challenge you on the pros and cons of both sides. "],
["a-methodology-for-agile-machine-learning.html", "5 A Methodology for Agile Machine Learning 5.1 Linear and Circular Tasks 5.2 A Two-Way Workflow for Development 5.3 User Stories 5.4 Using Kanban for Data Science 5.5 Scoping Tasks 5.6 The Product Owner Role 5.7 Monitoring the Process", " 5 A Methodology for Agile Machine Learning Now we have interpreted the Twelve Agile Principles in the data science setting we can explore what an Agile data science workflow might look like. Let us remind ourselves that the workflow is always a means to an end. The Agile values and principles are the guidelines and the workflow should serve following the values amd principles the best you can. If at any moment in a project the team feels the workflow is no longer the optimal way to make decisions in an Agile way, it should change it. Therefore, this chapter should be considered as nothing more than an exploration, a bunch of thoughts. If some of it does not work for you for whatever reason, by all means find a better way. 5.1 Linear and Circular Tasks The tasks (or stories) in Agile software development are what I call linear tasks. They come from prodcut feature requets by stakeholders, collected by the product owner. The envisioned outcome is captured in a user story. The team translates it into the technical tasks and starts working on it. Both Scrum and Kanban do not prescribe the steps a task should go through, but it typically looks like the following. Figure 5.1: Linear flow in software development These type of tasks lend themselves well for scoping and committing oneself to what the product will look like in a few weeks time, as is done in Scrum. Building a product upon relationships in data results in what I call circular tasks. The starting position is the latest version of the product. If one has not yet a good idea how to further improve it we would typically do an explorative analysis. From this a hypothesis can be generated of how the product might be improved, which is subsequently tested. Next we can evaluate if we want to update the product based on the hypothesis tested. Both when updated and when not updated we start a new cycle after the task is completed. Figure 5.2: Circular flow in machine learning Data science projects also encompass linear tasks, such as setting up a pipeline to import the data and do basic data wrangling, creating apps or exposing the model results in an API. However, the circular nature of leveraging relationships between variable makes the highly structured Scrum method unfit for most data science projects. We simply cannot guarantee that a statistical model or a machine learning algorithm will be improved in two weeks time, because we don’t know if the hypotheses we are going to test will give us anything. Moreover, during the completion of a task we can generate new hypotheses that seem more relevant than the one we originally had scheduled to do after this task. If we had commited to a set of tasks for a fixed time period from which we cannot deviate, we are slowed down because we cannot directly act on newly gained insights. Data science encompasses a broad array of different project types. Maybe some don’t have circular tasks at all and are basically pure software development projects. Examples might be building an app on a known data source or automating reports to be sent every month. For these type of projects it might make sense to follow a Scrum like methodology, cutting up work into user stories that are implemented in fixed time intervals. Most data science projects, however, will have a mixture of linear and circular tasks. For any project with circular tasks the flexibility of Kanban is preferred over the highly organised Scrum. In the remainder of this text we assume projects to at least partially build on relationships that are unknow at the start of the project, and thus containing circular tasks. If this does not apply to your projects, you might find literature on regular Agile software development more appropriate. 5.2 A Two-Way Workflow for Development The subdivision of tasks into linear and circular ones connects with a two-way workflow for data science product development. This workflow makes a hard cut between the data product and exploratory research. The data product is the software that gets productionised. It is standalone, which means that it contains all the steps to go from source data to the end result. It can be a pipeline that contains loading, wrangling, preprocessing, modelling, and exposing steps. Or it can be the querying of different databases, joining the different data, calculating statistics and building plots, wrapping everything in a Markdown report that gets sent different stakeholders. No matter what the product looks like, it should be high quality software so we can rely on it. This will make results completely reproducible and automatable, the two requirements for continuous delivery of improved products. Exploratory research will greatly benefit from having an data product, because intermediate results on which to further explore quickly available are inmediately available. In order to test hypotheses quickly, exploration scripts can be interactive analyses without software requirements or even being reproducible in later stages. They should quickly give an indication if the tested hypothesis could improve the current product. 5.3 User Stories In both Scrum and Kanban the tasks ahead are formulated in user stories, clearly stating what the benefit of the customer will be once the user story is completed. This will retain focus on delivering customer value with everything you do. Also the work in a large data science project should be chunked up, but it is often difficult and even futile to formulate the task ahead in the shape of user stories. In software development there is an abundance of ways to change the product as it currently is. We could leave everything the way it is, but change this one little thing that will improve user experience. Say, giving one extra option at a dropdown or siging up for a newsletter. In data science, especially machine learning or statistical modelling, the final product is static, such as a model or a bunch of prediction. If you try to define a user story it will always go “As the I want to have the best possible model, such that ”. On a task level, as a rule of thumb, user stories makes sense for linear tasks, but not for circular ones. It does make sense, though, to create an overarching user story for the entire project. This will enforce the team and the stakeholders to envision the final product and exactly how it is going to be used. All too often data science projects are initiated with an explorative nature, to “see if there is anything in the data”. If the group does not manage to formulate a proper user story for the project, there is probably no good application for the model and you should not start exploring in the first place. 5.4 Using Kanban for Data Science We have concluded that Scrum is too rigid for a data science project with circular tasks, because the explorative nature of data analysis is not suitable for the tight planning of deliverables. Kanban on the other hand gives us the flexibility to change the next task we are taken upon ourselves. Within a two-way model for doing data science there is the data product that has to be good quality software and there is the explorative research in which you can do whatever to come to quick conclusions. It appears that the linear tasks of software design are a match with building the data prodict, whereas the circular tasks are applicable to the exploratory research. Weaving these two types of work together results in a Kanban workflow with at least the columns to do, doing, and done. Both hypotheses to be tested and planned work on the data productare gathered in the to do column. This is the backlog and it is always ordered from most to least urgent, so it is clear what to do next. Kanban gives focus, finishing one task at a time. Too often when doing data science we have interesting finds on which we jump right away without finishing what we were doing. To prevent that, just add the new find as an hypothesis to the board. This will make sure that the tasks that are currently in doing always gets completed first, and that after each completion there is a moment where it can be decided what is the most urgent change to the pipeline or the most promising hypothesis to explore next. As a rule of thumb, never work on more tasks simultaneously than that there are data science members on the team. Tasks are either software or research tasks, which directly indicates requirements there are to the completion of the task. If the research task results in a proposed model update, the update can be captured in a newly formed software task (which can be placed on top of the to do list right away). 5.5 Scoping Tasks Scrum uses story points to scope its stories. The team itself determines the number of points awarded to each story, it does so in the sprint planning session. The team knows how many story points it typically completes in a sprint, so after scoping the sprint can be planned by selecting stories such that the total of their points does not exceed the team’s capacity. Kanban does not scope stories. In fact, the average time of taks completion is Kanban’s key metric of effectiveness. When doing data science with Kanban it might still be valuable to scope the tasks ahead, especially for exploring hypotheses. One of the major pitfalls of trying to improve a data science products is endless exploration of an hypothesis. We like to have just one more look from this other angle, or maybe this new fancy algorithm that you are anxious to test for a while will give a major boost. Data scientists are typically assiduous by nature, this is what allows them to master a wide range of dificult topics from statistics to programming in the first place. This could lead to stubbornes, however, unwilling to give up what was thought the way to get a major improvement. Scoping for data science is then not just estimating how long a task will take to complete, it is also time boxing. If used in this way, the scoping should be done in time units, not in a subjective measure such as story points. The data scientist should not take longer for the task than the team agreed upfront, wrapping up even when he does not feel completely fihished. If he found an alleyway that is still worthwhile exploring a new task should be put in the backlog, instead preservering in the current task. Scoping also helps with prioritising. If there are several candidate tasks to do next, the one with the least time to complete might be best done first. 5.6 The Product Owner Role When doing software development with Kanban there is typically a product owner involved. She aligns with customers and stakeholders, and adds the feature requests to the to do column of the board. For data science engineering it can also be desirable to have someone else than the data scientist doing stakeholder management and communication of the model results. This will free up time and energy for model development. Gathering the tasks to do cannot be primarily lay at the product owner. The data scientist will probably post most of the tasks on the Kanban board, because both hypotheses to test and maintenance work on the data product typically require a in depth knowledge. Product owners can add feature requests to the data products, especially when the product has a large software component, such as a Shiny application. You should discuss the tasks you put on the Kanban board with the product owner, even when their technical. This will demistify the model building and makes sure she can do a better job explaining the work to stakeholders. Especially when you are the sole data scientist on the project she also needs to get involved in prioritizing and scoping. Discussing how much time it will cost to complete the task and what it would bring can lead you to more accurate estimates of the time and value of the task. Also, the product owner might raise concerns from the business side that you did not think about, leading to a different prioritisation. 5.7 Monitoring the Process Reflecting on your workflow is a key element of Agile. Are you still continuously delivering? Are you still aligned with the business? Is there a better way of scoping tasks then you are doing now? How you reflect on this is also you have to find out for yourself as a team, but it might make sense to use the biweekly rythm of Scrum. You might combine it with scoping tasks that were put on the board the lately and reprioritizing your work. "],
["code-organsation.html", "6 Code Organsation 6.1 Using R Packages 6.2 Further Reading", " 6 Code Organsation The essence of Agile Data Science, continuously delivering an uptaded product, can only be achieved when the code is of high quality. Many data scientists, especially those who are drawn to R, have a background other than software development. Writing software is something we do to solve problems in our professional field, such as statistics, ecology, actuary or one of the many other fields R is used. The code is typically written interactively with the data. Write a a few lines of code, run the data set at hand, check if the results match your expectations, if it does go on and write a few more lines of code. This approach might be fine for a quick explorative data analysis in which you write code to answer a few ad hoc questions. It is not fine for large scale projects in which the data prodcut should be reliable and stable, because this way of working results in low reproducibility and flexibility towards new situations. Reproducibility is low because code is not developed as part of an end-to-end product, but incrementally in which each time a bit of code is added. We could call this the random walk programming, the next step is determined only by the current state of the code and data. Code developed this way only keeps working when downstream code and data are staying exactly the same. The larger the project becomes the less likely this condtion is met. You can say that we are code overfitting on the data. The code only works on a particular situation, as soon as there is a small change the code breaks. Oftentimes this way of developing is combined with saving intermediate results, to prevent having to run all the code time and time again, especially when some heavy calculations are involved. Together they can be a reproducibility recipe for disaster. The code never runs end-to-end, but it quickly is unclear which intermediate results are out of sync. It gets unclear which reported results are obtained on which data set and if this was before or after a modification to the code. Working in this way will create uncertainty, stress and an unreliable product. We cannot build an Agile workflow on such basis. 6.1 Using R Packages To remedy this high stress, low reproducibility workflow we should turn to the best practises of software development. Luckily, software development is not foreign to R, it provides the package structure that enables users to store their own function and share them with others. Although R has ample opportunities for doing object oriented programming, the most used and suitable structure for building complex data products is the function. The ADSwR approach to code organisation is to build a data product that solely consists of functions. Lower level functions are combined to form more complex operations. These operations are then chained together to create the entire product, a pipeline through which the data flows from the source to the final predictions, reports or dashboards. In the subsequent chapters we will explore several aspects of building a high quality pipeline, that gives reproducible and reliable results. 6.1.1 Explorative Research In the previous chapters it was concluded that the circular nature of explorative data analysis makes data science projects fundamentally different from ‘regular’ software design. Whereas the data product should be of high quality, exploration should be done quickly to indicate if we can advance the project by incorporating the results of the analysis. R packages have Vignettes, documents in which the package authors show how to use the package, such as this one. One of the ways of creating such a Vignette is by using Rmarkdown documents. These happen to be ideal for data exploration, combining code in the code blocks with comments and observations in the text around it. The functions from the package are readily available here. 6.2 Further Reading Hadley Wickham wrote a comprehensive book on developing software in R packages. It is freely available online, here. "],
["code-that-fails.html", "7 Code that Fails 7.1 Assumptions to your Data 7.2 Unit Testing", " 7 Code that Fails Your code will fail. Not once, not twice, but hundreds, thousands of times in the course of a large data science project. Everybody’s code fails, many times. Your code, my code, Hadley Wickham’s code. Not because we are bad at what we do, but because it is not possible to foresee all the possible ways the code will be used or all the possible data inputs that the code will run against at the moment we write it. Getting better at programming does not mean you will write fault-free code (although you will make fewer errors as you grow more experienced), but writing code that fails fast and informatively. Writing such code means that you will spent more time developing than with the random walk method described in the previous chapter. This is an investment that will pay off big time as your project grows in complexity. But before we look into how to write code that fails fast and informative, why is this important for Agile Data Science? High quality code is part of the twelve principles, but this is a means to an end. The essence of Agile is continuous delivery. Updating and shipping your product everytime you made progress can only be done when your code is adjustable. As the project advances, the end-to-end product will grow in complexity. Sooner rather than later it will reach a point in which you cannot have a full overview of all the aspects of the product anymore. When introducing a new feature you want to make sure all the other elements in the code base keep doing what they were designed for. If not, you want to be instructed clearly what is going wrong so you can either adjust the newly introduced feature or modify the existing elements so it works with the new feature. If everytime you want to make a change the whole thing comes tumbling done without informing you what goes wrong, you cannot achieve the objective of fast, continuous delivery. 7.1 Assumptions to your Data Automation means not calling the functions yourself anymore. You just flip the switch and the whole thing is set in motion. This means that you are no longer get the inmediate feedback you are used to when calling the functions yourself. When something goes wrong the whole thing just breaks with an error message. When you are lucky the error message is informative and you are quick to find the bug. When you are not, you may be confronted with object of type 'closure' is not subsettable, you may have grumpy afternoon ahead. Best to not depend on luck, but make sure you will always get a proper indication what went wrong. 7.1.1 Type Checking Most languages using functional programming are strongly typed. This means that for every argument that a function takes, its datatype is specified at function creation, as well as the datatype that the output takes. When the function is than called it is first checked if the data on which it is called is of the correct type, or at least can be forced to the specified type. If not, it will break inmediately and will tell the user why. R is weakly typed, the data type of the function arguments are not specified. When a function is called it just has a go on the objects that are fed to it. The function only breaks when somewhere in the body another function gets called with an invalid data type. Take the following for instance: add_two_numbers &lt;- function(x, y) { print(&quot;Reaching this&quot;) print(&quot;and this&quot;) print(&quot;Doing some random other stuff&quot;) x + y } Now, what will happen if we accidently call it on a string? It will only break when we hit the + operator, all the code before it runs. The error message it gives us, is not so informative that we inmediately figure out what goes wrong. add_two_numbers(42, &quot;MacGyver&quot;) ## [1] &quot;Reaching this&quot; ## [1] &quot;and this&quot; ## [1] &quot;Doing some random other stuff&quot; ## Error in x + y: non-numeric argument to binary operator Pfff, what is a binary operator again? When this functions is part of a framework in which higher order functions call lower order functions you may be up for an hour or two of sifting through your code to locate the bug. Fortunately type checking is very easily add to a function by asserting all argument data types in stopifnot. add_two_numbers &lt;- function(x, y) { stopifnot(is.numeric(x), is.numeric(y)) print(&quot;Reaching this&quot;) print(&quot;and this&quot;) print(&quot;Doing some random other stuff&quot;) x + y } add_two_numbers(42, &quot;MacGyver&quot;) ## Error in add_two_numbers(42, &quot;MacGyver&quot;): is.numeric(y) is not TRUE We are now specifically informed, which argument does not meet the assumptions and in what the name of the subfunction is. 7.1.2 Column Validation Central to most data products will be the modification of data frames. Probably most functions you’ll create are very specific to your project and will be called only once. For these cases you want to save yourself the overhead of making each function completely generic by parametrising the column names that are used. Say you want to add the log of the target to the data frame and you create the following function. add_log_target &lt;- function(x) { stopifnot(is.data.frame(x)) x$target_log &lt;- log(x$target) x } Now what if target was mistakingly removed from x in the step preceding this one in the product. Will it throw an informativer error? add_log_target(mtcars) ## Error in log(x$target): non-numeric argument to mathematical function Uhh, no not exactly. Another of those R error beauties that will leave you clueless for a time. It is a good idea to check if all the columns that are required for the operations in the function are present before starting them. I use this little function, df_has_cols &lt;- function(x, cols, func) { stopifnot(is.data.frame(x)) if(!all(cols %in% colnames(x))) { not_present &lt;- setdiff(cols, colnames(x)) stop(paste(not_present, collapse = &quot;, &quot;), &quot; missing from the data frame in function &quot;, func[[1]]) } } which is added to each function that uses a data frame add_log_target &lt;- function(x) { df_has_cols(x, &quot;target&quot;, match.call()) x$target_log &lt;- log(x$target) x } add_log_target(mtcars) ## Error in df_has_cols(x, &quot;target&quot;, match.call()): target missing from the data frame in function add_log_target (match.call() has to be added so it will return the name of the function that breaks for easy debugging). 7.1.3 Data Validation Where the first two checks are about the structure of the inputs, data validation tests the assumptions to the data itself. Variables that cannot contain missing values, variables that must be strictly positive, or characters that can only take a limited number of values. If you have any such assumptions to your data, you do best to check them before you unleash your functions on it. You could use external packages such as validate or recipes, or you can write them yourself. The stopifnot function can be used to check any assumption you have, as long as you can create a single logical for it. In the above example, since we are taking the log of target and it is the target variable it must be strictly positive and nonmissing. add_log_target &lt;- function(x) { df_has_cols(x, &quot;target&quot;, match.call()) stopifnot(all(!is.na(x$target)), all(x$target &gt; 0)) x$target_log &lt;- log(x$target) x } 7.2 Unit Testing The above, type checking, column validation, and data validation, test assumptions to the data going into the functions. To data scientists learning about these usually makes a lot of sense, because most who worked on a larger product without these has been bitten quite a bit. Moreover, these measures are quick to implement by just inserting one or two lines of code at the beginning of a function. Unit testing your code does not come so natural to data scientists, unless they have a background in software development. Writing tests might seem as a pointless, time consuming exercise at first sight. This is because its benefits are not inmediately obvious to someone who never applied used them. Still, you should make it part of your software development routine, even when you are the only one using the software you write. If you are unsure what unit tests are or how to do them in R, please read the chapter in R packages carefully. 7.2.1 Externalising Assumptions Reading someone else’s code is demanding, even when written by an excellent programmer. At the moment of writing the programmer is fully submerged in the problem, accommodating for all the inputs she foresees the code can be confronted with. Reading her code is tough because you have to recover from it what the problems are it tried to solve. After you wrap up a part of the project and move on to work on something else, the wrapped up code quickly becomes as if written by someone else. Unit tests capture all the cases the programmer envisioned the unit of code should handle. It is a service to the future collaborator, who very well could be the same person, so she doesn’t have to mentally fight her way into the code. As soon something is changed to the function such that it no longer does everything it was designed to do, the unit test informs us. A workflow without unit tests means that changes to code needs to be interactively checked against again, as was done when the code was written. The code fails, but it might be unclear why it fails, leaving the programmer with quite a puzzle. An even worse scenario would be if the code does not fail, for now. The exception that should be handled happens not to be in the data that is used to interactively check the modification. All seems fine and the code is adjusted. Then when the edge case does appear again in the data the product breaks mysteriously. From the above it should be clear that to work in an Agile way automated code testing should be in place. Agile’s core objective, continuous delivery, can only be obtained if modifications to the product can be made quickly. Omit testing will result in fingers crossed programming, which is neither effective nor fun. At first it might seem that skipping unit tests will get you to deliver faster. However, as time progresses and the product grows in complexity, a unit tester will be able to keep the same steady pace in delivering new features, where the system of those who cut corners will start to be grinding to a halt because they have to keep digging in old code that no longer works because of the newly introduced features. In the visual representation of building a data science prodcut below each feature to be added is represented by a color. On top the data scientist who does unit test, below the one who doesn’t. The unit tester takes quite some time to implement the frist two features, the non-unit tester delivers them much quicker. After implementing the third feature, however, some code that was written for the first feature keeps failing and he doesn’t know why. When the fourth is implemented, these problems arise again, in a slightly different form. When he finally manages to control it the code of the fourth feature gets unstable, asking for his attention. Meanwhile, the unit tester soldiers on. Sometimes she needs to make small adjustments to the old features, to make them click with new ones. In general however, she can uphold Agile’s promise of continuous, equally paced, delivery. Of course this example is contrived. After adding a new feature it gradually blends into the code base of the product. You will probably not say “I am going to revisit feature 2 now”, but rather “I will look into this error that keeps showing up now and then when we run it on new data”. Still, I think it is a good idea to keep this picture in your head as a reference, use unit tests and you won’t be solving the same problems over and over again. 7.2.2 Writing Better Code An additional benefit of unit testing is that it will improve the code quality directly. A point also made by Hadley Wickham in the first section of the chapter on unit testing. In order to be unit testeable the code should comprised of many small parts, instead of big chunks. Like a broken machine that is easier to fix when components can be replaced separately, code comprised of many small components is easier to debug, optimise and maintain. The main reason people are not eager to write unit tests, because at first site it seems redundant and time consuming work. I hope by now you are convinced it is certainly not redundant. As you get the hang of it, you will notice that being slowed down does actually make you stop and think. What are the edge cases that this function can mee and how do you want to behave it when it does? By thinking through the scenarios before you put the function to work you can circumvent many problems up front. "]
]
