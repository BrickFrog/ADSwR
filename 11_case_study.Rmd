# Case Study: Building the *Valuecheck*

My current employer (funda)[https://www.funda.nl/] is the market place for selling and buying homes in the Netherlands.
A long standing company wish was to use the data of recent house sales for predicting the current value of all houses in the Netherlands.
We knew that home owners used the asking prices of neighboring houses published on *funda* to keep track of local market trends.
We wanted to facilitate them to translate the recent sale prices in an official estimate of their specific house.
They no longer had to look at the houses that are offered, out statistical model had already done that.
Moreover, they did not make the translation of offered houses to their own house informally, the model determined which characteristics of a house matters and which not.
A final advantage was that we could use of the selling prices instead of the asking prices, these are not shown on the website.
So the product reflected what their house could be sold for, instead of what the typical asking price of their home was.

To create what eventually would become the *Valuecheck*, a colleague data scientist and me joined an existing Scrum team.
This team comprised of two backend developer, a frontend developer, a UX designer, and a product owner. 

## Trying Scrum

The teams was very experienced with using Scrum and had all the workflows in place, so it made sense to try to fit in our tasks into this framework.
At first this worked out quite well, because the first set of tasks we had to complete were essentially software tasks.
Setting up a server, build a first query so we had a modelling set, splitting into train and test, and doing some data cleaning.
These tasks were scopeable, we could estimate the time we needed to complete them quite accurately and they had a clear definition of done.
Then the model building started and we got more and more trouble fitting the tasks into the tight Scrum methodology.
We could not tell what the model would like in two weeks time, it depended on the relationships we would find in the data.
We certainly could not give estimates what the model quality would be then (measured in a an agreed-upon statistic).

## Moving to Kanban

Having scopeable tasks is essential for building proper Scrum sprints.
As a team you have to commit to what you are going to complete in the upcoming two weeks.
No longer being able to do that, we could not be part of the Scrum rythm anymore.
We found the alternative in moving the data science tasks to a separate Kanban board, stepping out of the Scrum cycles.
The circular nature of data science, as discussed in Chapter 5, does not lend itself well for tight planning. 
We started with a Kanban board with six lanes *to do* - *test hypothesis* - *code review hypothesis* - *update model* - *code review update model* - *done*.
This reflected a full cycle of researching a hypothesis and updating the software. 
It worked quite well, however, a part of the tasks did not reach the finish line, because the hypothesis tested appeared not to improve the model.
This problem does not exist if the research part and the software part are split in separate tasks, the software task is only created if the research indicated the model could be improved. 
We never did this during this project, but this insight improved our workflow in subsequent projects.

## Building an MVM for the MVP

Building a predictive model that is part of a dedicated product is both challenging and rewarding.
Too often data science projects are initiated as a proof of concept, without a clear vision on how to implement if the prediction can be succesfully done. 
Knowing from the start that the model is going to be used, is very motivating.
On the other hand, this means that you need constant allignment with the team that develops the product around the predictions.
The houses offered for sale on *funda* have many charecteristics filed, giving us a rich feature set to work with.
However, as an MVP we wanted to present the users with an estimation, without them having to fill in all kinds of characteristics of theirs house.
Developing a product from static house predictions is far less complex and time consuming than from a dynamic model with adjustable inputs.
Both from modelling and a software perspective.
This implied that we could only use features that are freely available for every house in the Netherlands.
Luckily, this was true for the two most important features, location and time.
Also the surface area of the houses was available in a public database.
From this we started to build our initial prediction models.
First using simple regression model to create a baseline.
We have a strong preference for statistical models over machine learning algorithms, because they not only give us predictions, but also insight.
However, we needed some decent predictions fast, and it was clear we needed to exploit some nonlinear relationships.
We therefore used some ensemble models that gave far superior predictions to regular regression.

Already it was decided, we would only release the MVP in geographical areas in which the MVM performed well enough.
Even then we not quite made the minimal performance goals we set ourselves. 
However, we could include a categorical feature and simply export the predictions for every level of the feature for every house, as long as there were not too many levels.
The type of the house (appartment, on of several Duthch types of houses) appeared another crucial predictor.
Finally, we wanted to show lower and upper bounds to a prediction, not only giving a point estimate.
After some research we were able to this with random forests, that were trained on the desired quantiles.
Predictions were exported in csv files, a frontend and backend were built around these.
Doing a prediction on the website was just a simple lookup.

## Improving the Product




